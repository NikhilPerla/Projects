{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 24 Spring CISC 484/684 Assignment 1 - Analyzing Paragraph Similarity with BERT Embeddings on Google Colab\n",
        "\n",
        "\n",
        "## Objective\n",
        "This assignment aims to familiarize you with the use of pretrained BERT (Bidirectional Encoder Representations from Transformers) for generating sentence embeddings and comparing their similarities. By leveraging Google Colab and the HuggingFace transformers library, you will produce embeddings for selected paragraphs and compute dot-product and cosine similarity measures to evaluate the model's ability to discern contextual similarities and differences."
      ],
      "metadata": {
        "id": "jFHcPK3XInfv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part I: Set up the Environment\n",
        "\n",
        "1) To begin your assignment, please create your own version of this Colab notebook. Click on \"File\" at the top of the menu bar, then choose \"Save a copy in Drive\". This will generate a personal copy of the notebook in your Google Drive where you can complete the tasks. Upon finishing your homework, please turn in both a downloaded version (in .ipynb) and include a shareable link to your Colab notebook for review and grading. This will be elaborated at the end of this notebook.\n",
        "\n"
      ],
      "metadata": {
        "id": "RW0VoLe0JQqp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2) The first task is to install [HuggingFace Transformers library](https://huggingface.co/docs/transformers/en/index). The provided link is the official documentation of the HuggingFace `transformers` library, in which it displays the other transformer models that this library supports and provides more tutorials about using this library.\n",
        "\n",
        "Please execute the following cell to install HuggingFace's `transformers`."
      ],
      "metadata": {
        "id": "XR6EryzAKqDj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPnbzoMrIluw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55437553-cee1-4d4e-a8bd-d0d754a356d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) After installing, import the BERT model from the transformers library and Tensorflow or PyTorch framework of your choice.\n",
        "\n",
        "3.1) If you are more familiar with Tensorflow, you may import your libraries by executing the following cell. Click [here](https://huggingface.co/docs/transformers/en/model_doc/bert#transformers.TFBertModel) to learn more about `TFBertModel`."
      ],
      "metadata": {
        "id": "wl4OTWyeK2tz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertModel"
      ],
      "metadata": {
        "id": "fN94Z8Z0L07R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2) If you are more familiar with PyTorch, you may import your libraries by executing the following cell. Click [here](https://huggingface.co/docs/transformers/en/model_doc/bert#transformers.BertModel) to learn more about `BertModel`."
      ],
      "metadata": {
        "id": "veGnd4dsJEl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertTokenizer, BertModel"
      ],
      "metadata": {
        "id": "DTKo4jyJMQSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part II: Choose Your Paragraphs\n",
        "\n",
        "Please choose three paragraphs for comparison, each consisting of 1-3 sentences. The first paragraph will serve as the base. The second should be contextually similar to the first, and the third should differ significantly in context from the first."
      ],
      "metadata": {
        "id": "9MwyVNAfMXqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# base paragraph\n",
        "paragraph_base = \"Chennai Super Kings is a professional cricket franchise based in Chennai, Tamil Nadu, India, that competes in the Indian Premier League\"\n",
        "\n",
        "# Contextually similar paragraph\n",
        "# paragraph_similar = \"Chennai Super Kings won five titles in Indian Premier League\"\n",
        "paragraph_similar =  \"Mumbai Indians is a professional cricket franchise based in Mumbai, Maharastra, India, that competes in the Indian Premier League\"\n",
        "\n",
        "# Contextually different paragraph\n",
        "paragraph_different = \"My name is Nikhil\""
      ],
      "metadata": {
        "id": "qdaCJYJnNOWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part III: Generate Embeddings and Compute Similarities\n",
        "\n",
        "Prior to generating embeddings, it's essential to convert your paragraphs into tokens, as BERT requires tokenized input. Begin by loading the pretrained BERT tokenizer. To learn more about the [tokenizer](https://huggingface.co/docs/transformers/v4.38.1/en/model_doc/bert#transformers.BertTokenizer) and the [BERT base model (uncased)](https://huggingface.co/google-bert/bert-base-uncased), click the links to the corresponding documentations."
      ],
      "metadata": {
        "id": "EY3oWamISGfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "rMbU0tpVSHX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subsequently, tokenize your paragraphs as the first step in the Tensorflow or PyTorch sections below."
      ],
      "metadata": {
        "id": "7BMO9j8delVn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Tensorflow\n",
        "\n",
        "If you choose Tensorflow as your deep learning framework, follow the below cells. If you are using PyTorch, you may skip to the PyTorch section after Tensorflow.\n",
        "\n"
      ],
      "metadata": {
        "id": "HA2larltNpxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Tokenize your paragraphs by following the instructions in the below cell.\n",
        "\n",
        "Padding adds special tokens to shorter sequences to match the length of the longest sequence in a batch (or a specified maximum length), and truncation shortens sequences longer than a specified maximum length, which defaults to the model's maximum input size, and in this case, [512 tokens for BERT base model](https://github.com/mim-solutions/bert_for_longer_texts). Basically, they are techniques used to handle sequences of varying lengths in batch processing. To learn more about padding and truncation, refer to the [documentation](https://huggingface.co/docs/transformers/en/pad_truncation).\n"
      ],
      "metadata": {
        "id": "RiEaDfFEY6kv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input_b = tokenizer(paragraph_base, padding=True, truncation=True, return_tensors='tf')\n",
        "encoded_input_s = tokenizer(paragraph_similar, padding=True, truncation=True, return_tensors='tf')\n",
        "encoded_input_d = tokenizer(paragraph_different, padding=True, truncation=True, return_tensors='tf')"
      ],
      "metadata": {
        "id": "SyHDOP_8ZRaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Load the [BERT base model](https://huggingface.co/google-bert/bert-base-uncased) for TensorFlow:\n"
      ],
      "metadata": {
        "id": "oslIIoYSRBJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_tf = TFBertModel.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "No_u8N-tRAc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "033e5b33-073f-40ca-d21c-e9397d3105f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Get model outputs"
      ],
      "metadata": {
        "id": "8i7oi_5bpRVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs_tf_b = model_tf(encoded_input_b)\n",
        "outputs_tf_s = model_tf(encoded_input_s)\n",
        "outputs_tf_d = model_tf(encoded_input_d)"
      ],
      "metadata": {
        "id": "DzSMU1nrpUZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output from the model call `model_tf(encoded_input)` typically includes several components for BERT models:\n",
        "\n",
        "\n",
        "1.   `last_hidden_state`: The final layer's hidden states for each token in the sequence. For each token, this is a vector representing the token's contextual embedding.\n",
        "2.   `pooler_output`: The output of a pooling operation applied to the last hidden state of the [CLS] token, often used for classification tasks.\n",
        "3. Optionally, `hidden_states` and `attentions`: If the model is configured to return these, they represent the hidden states and attention weights from all layers of the model, respectively.\n",
        "\n",
        "The `last_hidden_state` tensor can be used to obtain token-level embeddings, which we will explore next."
      ],
      "metadata": {
        "id": "4pfkTWD1pVQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Obtain the embeddings\n",
        "\n",
        "Here, we examine two different embedding methods\n",
        "\n",
        "1. Using the mean of the last hidden state outputs across the sequence length dimension to generate a sentence-level embedding (the mean pooling method). This approach averages the token embeddings produced by the BERT model, providing a single vector that represents the entire input sequence.\n",
        "\n",
        "2. Using the [CLS] token embedding, which is specifically designed to capture the context of the entire sequence for classification tasks. The [CLS] embedding is the first token's output from the last hidden layer, directly intended for sequence-level tasks, while the mean pooling method combines information from all tokens to form a representation. While the [CLS] embedding is commonly utilized in classification tasks, its capability to encapsulate sequence context also allows for its application in similarity comparisons between texts. It's important to realize this method, especially as we delve into text classification topics like sentiment analysis.\n",
        "\n",
        "After knowing the [CLS] token embedding method, going back to the mean pooling method, there are two ways of doing it.\n",
        "\n",
        "(1) One way is to computes the mean across the sequence dimension (axis=1) excluding the first token of each sequence. This is useful when you want to ignore the special [CLS] token in BERT's output when calculating the average embedding. The [CLS] token is often used for classification tasks, and excluding it might be preferred for tasks where only the content tokens' embeddings are relevant."
      ],
      "metadata": {
        "id": "pNveYpcWSevI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mean pooling method that excludes the [CLS] token\n",
        "\n",
        "# embeddings_mean_b = tf.reduce_mean(outputs_tf_b.last_hidden_state[:, 1:, :], axis=1)\n",
        "# embeddings_mean_s = tf.reduce_mean(outputs_tf_s.last_hidden_state[:, 1:, :], axis=1)\n",
        "# embeddings_mean_d = tf.reduce_mean(outputs_tf_d.last_hidden_state[:, 1:, :], axis=1)"
      ],
      "metadata": {
        "id": "cVEM6bF6psjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2) The second common way is to compute the mean across the sequence dimension (axis=1) including all tokens, i.e., the [CLS] token, and any other tokens present in the sequence. This might be useful when the context captured by the [CLS] token is also considered relevant for your task."
      ],
      "metadata": {
        "id": "32MS_VWpqCIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mean pooling method that includes the [CLS] token\n",
        "\n",
        "embeddings_mean_b = tf.reduce_mean(outputs_tf_b.last_hidden_state[:, :, :], axis=1)\n",
        "embeddings_mean_s = tf.reduce_mean(outputs_tf_s.last_hidden_state[:, :, :], axis=1)\n",
        "embeddings_mean_d = tf.reduce_mean(outputs_tf_d.last_hidden_state[:, :, :], axis=1)"
      ],
      "metadata": {
        "id": "ln_YNsEYqU-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In practice, the choice of which method may depend on trial and error and/or the specific needs of your task. In this homework, you may choose either of the above two cells to run to obtain the `embeddings_mean` from the mean pooling method to report your findings."
      ],
      "metadata": {
        "id": "yggxn3JPrubo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below cell provides code for obtaining the the [CLS] token embedding."
      ],
      "metadata": {
        "id": "gyl8p4kGr4iy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [CLS] token embedding\n",
        "embeddings_cls_b = outputs_tf_b.last_hidden_state[:, 0, :]\n",
        "embeddings_cls_s = outputs_tf_s.last_hidden_state[:, 0, :]\n",
        "embeddings_cls_d = outputs_tf_d.last_hidden_state[:, 0, :]"
      ],
      "metadata": {
        "id": "tlSUh0pVrrnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feel free to explore this [post](https://stackoverflow.com/questions/62705268/why-bert-transformer-uses-cls-token-for-classification-instead-of-average-over) for a deeper understanding of the utilization of the [CLS] token embedding in BERT and its distinctions from the mean pooling method.\n"
      ],
      "metadata": {
        "id": "Jjrpw_Q-pz1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) Compute dot-product and cosine similarity:"
      ],
      "metadata": {
        "id": "ax6XJQLZSyS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dot product and cosine similarity for mean pooling\n",
        "\n",
        "dot_product_mean_s = tf.reduce_sum(tf.multiply(embeddings_mean_b, embeddings_mean_s), axis=1)\n",
        "dot_product_mean_d = tf.reduce_sum(tf.multiply(embeddings_mean_b, embeddings_mean_d), axis=1)\n",
        "\n",
        "cosine_similarity_mean_s = tf.reduce_sum(tf.multiply(embeddings_mean_b, embeddings_mean_s), axis=1) / (\n",
        "    tf.norm(embeddings_mean_b, axis=1) * tf.norm(embeddings_mean_s, axis=1)\n",
        ")\n",
        "cosine_similarity_mean_d = tf.reduce_sum(tf.multiply(embeddings_mean_b, embeddings_mean_d), axis=1) / (\n",
        "    tf.norm(embeddings_mean_b, axis=1) * tf.norm(embeddings_mean_d, axis=1)\n",
        ")\n",
        "\n",
        "# Dot product and cosine similarity for [CLS] token\n",
        "\n",
        "dot_product_cls_s = tf.reduce_sum(tf.multiply(embeddings_cls_b, embeddings_cls_s), axis=1)\n",
        "dot_product_cls_d = tf.reduce_sum(tf.multiply(embeddings_cls_b, embeddings_cls_d), axis=1)\n",
        "\n",
        "\n",
        "cosine_similarity_cls_s = tf.reduce_sum(tf.multiply(embeddings_cls_b, embeddings_cls_s), axis=1) / (\n",
        "    tf.norm(embeddings_cls_b, axis=1) * tf.norm(embeddings_cls_s, axis=1)\n",
        ")\n",
        "cosine_similarity_cls_d = tf.reduce_sum(tf.multiply(embeddings_cls_b, embeddings_cls_d), axis=1) / (\n",
        "    tf.norm(embeddings_cls_b, axis=1) * tf.norm(embeddings_cls_d, axis=1)\n",
        ")\n"
      ],
      "metadata": {
        "id": "m9VR6MdejT0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6) Now you can evaluate these tensors to get the actual values"
      ],
      "metadata": {
        "id": "W-eY3SNuaju2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dot_product_mean_s = dot_product_mean_s.numpy()\n",
        "dot_product_mean_d = dot_product_mean_d.numpy()\n",
        "\n",
        "cosine_similarity_mean_s = cosine_similarity_mean_s.numpy()\n",
        "cosine_similarity_mean_d = cosine_similarity_mean_d.numpy()\n",
        "\n",
        "dot_product_cls_s = dot_product_cls_s.numpy()\n",
        "dot_product_cls_d = dot_product_cls_d.numpy()\n",
        "\n",
        "cosine_similarity_cls_s = cosine_similarity_cls_s.numpy()\n",
        "cosine_similarity_cls_d = cosine_similarity_cls_d.numpy()\n",
        "\n",
        "print(dot_product_mean_s, dot_product_mean_d)\n",
        "print(cosine_similarity_mean_s, cosine_similarity_mean_d)\n",
        "\n",
        "print(dot_product_cls_s, dot_product_cls_d)\n",
        "print(cosine_similarity_cls_s, cosine_similarity_cls_d)"
      ],
      "metadata": {
        "id": "ewlTd2qeamEy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1719c0b5-a1f6-4673-c4df-839448a1760b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[81.15794] [44.172375]\n",
            "[0.9235468] [0.49835715]\n",
            "[256.81693] [115.12463]\n",
            "[0.9825427] [0.4938733]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PyTorch\n",
        "\n",
        "1) Tokenize your paragraphs by following the instructions in the below cell.\n",
        "\n",
        "Padding adds special tokens to shorter sequences to match the length of the longest sequence in a batch (or a specified maximum length), and truncation shortens sequences longer than a specified maximum length, which defaults to the model's maximum input size, and in this case, [512 tokens for BERT base model](https://github.com/mim-solutions/bert_for_longer_texts). Basically, they are techniques used to handle sequences of varying lengths in batch processing. To learn more about padding and truncation, refer to the [documentation](https://huggingface.co/docs/transformers/en/pad_truncation).\n"
      ],
      "metadata": {
        "id": "FcY4ORUwTXPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input_b = tokenizer(paragraph_base, padding=True, truncation=True, return_tensors='pt')\n",
        "encoded_input_s = tokenizer(paragraph_similar, padding=True, truncation=True, return_tensors='pt')\n",
        "encoded_input_d = tokenizer(paragraph_different, padding=True, truncation=True, return_tensors='pt')"
      ],
      "metadata": {
        "id": "R9upbZ8ATe51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Load the [BERT base model](https://huggingface.co/google-bert/bert-base-uncased) for PyTorch:"
      ],
      "metadata": {
        "id": "NZ2QydtRZjD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "JgWlI50vZe4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Get model outputs"
      ],
      "metadata": {
        "id": "xvOsNAmJqTKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    outputs_pt_b = model(**encoded_input_b)\n",
        "    outputs_pt_s = model(**encoded_input_s)\n",
        "    outputs_pt_d = model(**encoded_input_d)"
      ],
      "metadata": {
        "id": "Hsn8VtNRqWJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output from the model call `model_tf(encoded_input)` typically includes several components for BERT models:\n",
        "\n",
        "\n",
        "1.   `last_hidden_state`: The final layer's hidden states for each token in the sequence. For each token, this is a vector representing the token's contextual embedding.\n",
        "2.   `pooler_output`: The output of a pooling operation applied to the last hidden state of the [CLS] token, often used for classification tasks.\n",
        "3. Optionally, `hidden_states` and `attentions`: If the model is configured to return these, they represent the hidden states and attention weights from all layers of the model, respectively.\n",
        "\n",
        "The `last_hidden_state` tensor can be used to obtain token-level embeddings, which we will explore next."
      ],
      "metadata": {
        "id": "uZxfBJZyqQVo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Obtain the embeddings\n",
        "\n",
        "Here, we examine two different embedding methods\n",
        "\n",
        "1. Using the mean of the last hidden state outputs across the sequence length dimension to generate a sentence-level embedding (the mean pooling method). This approach averages the token embeddings produced by the BERT model, providing a single vector that represents the entire input sequence.\n",
        "\n",
        "2. Using the [CLS] token embedding, which is specifically designed to capture the context of the entire sequence for classification tasks. The [CLS] embedding is the first token's output from the last hidden layer, directly intended for sequence-level tasks, while the mean pooling method combines information from all tokens to form a representation. While the [CLS] embedding is commonly utilized in classification tasks, its capability to encapsulate sequence context also allows for its application in similarity comparisons between texts. It's important to realize this method, especially as we delve into text classification topics like sentiment analysis.\n",
        "\n",
        "After knowing the [CLS] token embedding method, going back to the mean pooling method, there are two ways of doing it.\n",
        "\n",
        "(1) One way is to computes the mean across the sequence dimension (axis=1) excluding the first token of each sequence. This is useful when you want to ignore the special [CLS] token in BERT's output when calculating the average embedding. The [CLS] token is often used for classification tasks, and excluding it might be preferred for tasks where only the content tokens' embeddings are relevant."
      ],
      "metadata": {
        "id": "i5jd1iYqTlWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mean pooling method that excludes the [CLS] token\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     embeddings_mean_b = torch.mean(outputs_pt_b.last_hidden_state[:, 1:, :], dim=1)\n",
        "#     embeddings_mean_s = torch.mean(outputs_pt_s.last_hidden_state[:, 1:, :], dim=1)\n",
        "#     embeddings_mean_d = torch.mean(outputs_pt_d.last_hidden_state[:, 1:, :], dim=1)"
      ],
      "metadata": {
        "id": "Vv1gCjcYsIGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2) The second common way is to compute the mean across the sequence dimension (axis=1) including all tokens, i.e., the [CLS] token, and any other tokens present in the sequence. This might be useful when the context captured by the [CLS] token is also considered relevant for your task."
      ],
      "metadata": {
        "id": "ZfRH5WHnsS8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mean pooling method that includes the [CLS] token\n",
        "\n",
        "with torch.no_grad():\n",
        "    embeddings_mean_b = torch.mean(outputs_pt_b.last_hidden_state[:, :, :], dim=1)\n",
        "    embeddings_mean_s = torch.mean(outputs_pt_s.last_hidden_state[:, :, :], dim=1)\n",
        "    embeddings_mean_d = torch.mean(outputs_pt_d.last_hidden_state[:, :, :], dim=1)"
      ],
      "metadata": {
        "id": "Mvgzd4p8Tkb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In practice, the choice of which method may depend on trial and error and/or the specific needs of your task. In this homework, you may choose either of the above two cells to run to obtain the `embeddings_mean` from the mean pooling method to report your findings."
      ],
      "metadata": {
        "id": "THTCcm9asZvP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below cell provides code for obtaining the the [CLS] token embedding."
      ],
      "metadata": {
        "id": "-eLin_etsgsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [CLS] token embedding\n",
        "\n",
        "with torch.no_grad():\n",
        "    embeddings_cls_b = outputs_pt_b.last_hidden_state[:, 0, :]\n",
        "    embeddings_cls_s = outputs_pt_s.last_hidden_state[:, 0, :]\n",
        "    embeddings_cls_d = outputs_pt_d.last_hidden_state[:, 0, :]"
      ],
      "metadata": {
        "id": "ZqUJGnn4si7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feel free to explore this [post](https://stackoverflow.com/questions/62705268/why-bert-transformer-uses-cls-token-for-classification-instead-of-average-over) for a deeper understanding of the utilization of the [CLS] token embedding in BERT and its distinctions from the mean pooling method.\n"
      ],
      "metadata": {
        "id": "snSO-SGPsfFt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) Compute dot-product and cosine similarity:"
      ],
      "metadata": {
        "id": "HtD3udHQTzYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dot product and cosine similarity for mean pooling\n",
        "dot_product_mean_s = torch.sum(embeddings_mean_b * embeddings_mean_s, dim=1)\n",
        "dot_product_mean_d = torch.sum(embeddings_mean_b * embeddings_mean_d, dim=1)\n",
        "\n",
        "cosine_similarity_mean_s = torch.sum(embeddings_mean_b * embeddings_mean_s, dim=1) / (\n",
        "    torch.norm(embeddings_mean_b, dim=1) * torch.norm(embeddings_mean_s, dim=1)\n",
        ")\n",
        "cosine_similarity_mean_d = torch.sum(embeddings_mean_b * embeddings_mean_d, dim=1) / (\n",
        "    torch.norm(embeddings_mean_b, dim=1) * torch.norm(embeddings_mean_d, dim=1)\n",
        ")\n",
        "\n",
        "# Dot product and cosine similarity for [CLS] token\n",
        "dot_product_cls_s = torch.sum(embeddings_cls_b * embeddings_cls_s, dim=1)\n",
        "dot_product_cls_d = torch.sum(embeddings_cls_b * embeddings_cls_d, dim=1)\n",
        "\n",
        "cosine_similarity_cls_s = torch.sum(embeddings_cls_b * embeddings_cls_s, dim=1) / (\n",
        "    torch.norm(embeddings_cls_b, dim=1) * torch.norm(embeddings_cls_s, dim=1)\n",
        ")\n",
        "cosine_similarity_cls_d = torch.sum(embeddings_cls_b * embeddings_cls_d, dim=1) / (\n",
        "    torch.norm(embeddings_cls_b, dim=1) * torch.norm(embeddings_cls_d, dim=1)\n",
        ")\n"
      ],
      "metadata": {
        "id": "IBjXh-bATkS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6) Now you can evaluate these tensors to get the actual values"
      ],
      "metadata": {
        "id": "aRnVSJXtp64Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dot_product_mean_s = dot_product_mean_s.numpy()\n",
        "dot_product_mean_d = dot_product_mean_d.numpy()\n",
        "\n",
        "cosine_similarity_mean_s = cosine_similarity_mean_s.numpy()\n",
        "cosine_similarity_mean_d = cosine_similarity_mean_d.numpy()\n",
        "\n",
        "dot_product_cls_s = dot_product_cls_s.numpy()\n",
        "dot_product_cls_d = dot_product_cls_d.numpy()\n",
        "\n",
        "cosine_similarity_cls_s = cosine_similarity_cls_s.numpy()\n",
        "cosine_similarity_cls_d = cosine_similarity_cls_d.numpy()\n",
        "\n",
        "# Print results\n",
        "print(dot_product_mean_s, dot_product_mean_d)\n",
        "print(cosine_similarity_mean_s, cosine_similarity_mean_d)\n",
        "\n",
        "print(dot_product_cls_s, dot_product_cls_d)\n",
        "print(cosine_similarity_cls_s, cosine_similarity_cls_d)"
      ],
      "metadata": {
        "id": "x5vL9N3qp3Z5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bd04b78-6499-472b-cfe8-589b0bbf9260"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[81.15797] [44.172398]\n",
            "[0.92354697] [0.49835733]\n",
            "[256.81686] [115.12464]\n",
            "[0.9825426] [0.4938734]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part IV: Analyze Your Results\n",
        "\n",
        "1. For the embeddings between paragraph_base and paragraph_similar, how similar are these two paragraphs according to BERT?\n",
        "\n",
        "  The dot product between paragraph_base and paragraph_similar(means, cls) are 81.15797, 256.81286 which are higher compared to Dot Product between paragraph_base and paragraph_different and Cosine Similarity between paragraph_base and paragraph_similar(means, cls) are 0.92 and 0.98 respectively, which are very high. These higher values suggest that they are very similar to each other.\n",
        "\n",
        "2. For the embeddings between paragraph_base and paragraph_different, how similar are these two paragraphs according to BERT?\n",
        "\n",
        "  The dot product between paragraph_base and paragraph_different(means, cls) are 44.172398, 115.12464 which are lower compared to Dot Product between paragraph_base and paragraph_different and Cosine Similarity between paragraph_base and paragraph_similar(means, cls) are 0.498 and 0.494 respectively, which are low. These lesser values suggest that they are not similar(Different) to each other.\n",
        "\n",
        "3. What does this tell you about the nature of sentence embeddings and their use in understanding contextual similarity?\n",
        "\n",
        "  The results demonstrate that sentence embeddings, generated by BERT in this case, can effectively capture contextual similarity between sentences. Both dot product and cosine similarity provide insights into the similarity of the contextual representations of sentences. BERT embeddings consider not only individual words but also their surrounding context, enabling a nuanced understanding of similarity between sentences.\n",
        "\n",
        "4. Which embedding method, mean pooling or the [CLS] token, would you find more effective for this analysis? Please explain your choice.\n",
        "\n",
        "  In this analysis, both mean pooling and [CLS] token embeddings give great(true) results. However, the given paragraphs differ in context. So the [CLS] token embedding method tends to be more effective for this analysis. The results also show that [CLS] token embeddings gives Higher Similarity(0.98) compared to mean pooling.\n",
        "\n",
        "5. Please investigate the meaning and role of each dimension in the `last_hidden_state` output from the BERT model through comprehensive research. This investigation will deepen your understanding of how BERT processes and represents textual data. First print out the three dimensions by executing the following code\n",
        "\n",
        "  The three dimension output is (1, 26, 768). This means we have 1 batch, 26 tokens, and each token has a 768-dimensional vector representation.\n",
        "  26 tokens represents the largest size of the paragraph. BERT adds two special tokens (CLS and SEP). Here, paragraph_base has 24(21 words, 3 commas)tokens and 2 (CLS and SEP) are added, so makes to 26."
      ],
      "metadata": {
        "id": "YVIO9UD6UMLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for TensorFlow\n",
        "print(outputs_tf_b.last_hidden_state[:, :, :].shape)"
      ],
      "metadata": {
        "id": "NESDKpVNtgQQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a32fd061-8ad0-4622-fc36-7a574e7fa951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 26, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for PyTorch\n",
        "print(outputs_pt_b.last_hidden_state[:, :, :].shape)"
      ],
      "metadata": {
        "id": "41I3jm51ttGK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af1635b9-9460-44a7-c033-3bd6b11f8f59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 26, 768])\n"
          ]
        }
      ]
    }
  ]
}